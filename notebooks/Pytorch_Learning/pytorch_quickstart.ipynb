{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuickStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from the open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train = True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train= False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]:  {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309230  [   64/60000]\n",
      "loss: 2.298622  [ 6464/60000]\n",
      "loss: 2.281975  [12864/60000]\n",
      "loss: 2.274302  [19264/60000]\n",
      "loss: 2.262661  [25664/60000]\n",
      "loss: 2.222073  [32064/60000]\n",
      "loss: 2.232169  [38464/60000]\n",
      "loss: 2.196241  [44864/60000]\n",
      "loss: 2.185888  [51264/60000]\n",
      "loss: 2.154585  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 2.153669 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.162763  [   64/60000]\n",
      "loss: 2.159248  [ 6464/60000]\n",
      "loss: 2.100536  [12864/60000]\n",
      "loss: 2.115034  [19264/60000]\n",
      "loss: 2.077618  [25664/60000]\n",
      "loss: 2.000604  [32064/60000]\n",
      "loss: 2.035298  [38464/60000]\n",
      "loss: 1.951664  [44864/60000]\n",
      "loss: 1.951069  [51264/60000]\n",
      "loss: 1.882192  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 1.884238 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.915032  [   64/60000]\n",
      "loss: 1.893007  [ 6464/60000]\n",
      "loss: 1.774770  [12864/60000]\n",
      "loss: 1.812678  [19264/60000]\n",
      "loss: 1.717377  [25664/60000]\n",
      "loss: 1.654327  [32064/60000]\n",
      "loss: 1.677637  [38464/60000]\n",
      "loss: 1.574439  [44864/60000]\n",
      "loss: 1.598914  [51264/60000]\n",
      "loss: 1.492193  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.513764 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.578149  [   64/60000]\n",
      "loss: 1.550228  [ 6464/60000]\n",
      "loss: 1.401316  [12864/60000]\n",
      "loss: 1.470765  [19264/60000]\n",
      "loss: 1.359110  [25664/60000]\n",
      "loss: 1.343058  [32064/60000]\n",
      "loss: 1.357006  [38464/60000]\n",
      "loss: 1.279236  [44864/60000]\n",
      "loss: 1.317674  [51264/60000]\n",
      "loss: 1.215500  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.244455 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.319449  [   64/60000]\n",
      "loss: 1.306855  [ 6464/60000]\n",
      "loss: 1.144351  [12864/60000]\n",
      "loss: 1.245718  [19264/60000]\n",
      "loss: 1.123609  [25664/60000]\n",
      "loss: 1.141547  [32064/60000]\n",
      "loss: 1.161288  [38464/60000]\n",
      "loss: 1.097861  [44864/60000]\n",
      "loss: 1.142081  [51264/60000]\n",
      "loss: 1.055152  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.078053 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), '/Users/stevenyu/personal_projects/machine_learning_study/experiments/model.pth')\n",
    "print(\"Saved PyTorch Model State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"/Users/stevenyu/personal_projects/machine_learning_study/experiments/model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "- Tensors are specialized data structures similar to arrays and matrices.\n",
    "- PyTorch uses tensors to encode the inputs and outputs of a model as well as the model's parameters.\n",
    "- Tensors are similar to Numpy's ndarrays, but tensors can run on GPUs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to initialize a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.5526, 0.0444],\n",
      "        [0.5763, 0.8350]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.0397, 0.9881, 0.7501],\n",
      "        [0.9913, 0.3174, 0.0983]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-element tensors\n",
    "- If you have a one-element tensor you can convert it to a Python numerical value using item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders\n",
    "\n",
    "- PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allows for pre-loaded datasets and your own data.\n",
    "- Dataset: stores the samples and their corresponding labels\n",
    "- DataLoader: wraps an iterable around the Dataset to enable easy access to the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom torch.utils.data import Dataset\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor\\nimport matplotlib.pyplot as plt\\n\\n\\ntraining_data = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=ToTensor()\\n)\\n\\ntest_data = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=ToTensor()\\n)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- root : The path where the train/test data is stored\n",
    "- train : Specifies training or test dataset\n",
    "- download : Downloads the data from the internet if it's not available at root\n",
    "- transform & target_transform : Specifies the feature and label transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset for your files\n",
    "\n",
    "- A custom Dataset class must implement three functions: __init__, __len__, and __getitem__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __init__\n",
    "- The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms \n",
    "\n",
    "### __len__\n",
    "- Returns the numbe of samples in our dataset.\n",
    "\n",
    "### __getitem__\n",
    "- The __getitem__ function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the imageâ€™s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for training with DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "- Data does not always come in the format required for training. \n",
    "- We use __transform__ to perform some manipulation of the date to make it more suitable for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToTensor()\n",
    "- Converts a PIL image or Numpy ndarray into a FloatTensor and scales the image's pixel intensity values in the range [0., 1.]\n",
    "\n",
    "### Lambda Transform\n",
    "- Apply any user-defined lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Class\n",
    "\n",
    "To define a neural network we subclass nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logitis = self.linear_relu_stack(x)\n",
    "        return logitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of NeuralNetwork, and move it to the device, and print its structure\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# nn.Flatten layer converts each 2D 28x28 image to a contiguous array of 784 pixel values.\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flate_image = flatten(input_image)\n",
    "print(flate_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# The linear layer is a module that applies a linear transformation on the input using its stored weights and biases\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flate_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLu (a type of non-linear activation function)\n",
    "- The non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "- They are applied after linear transformations to introduce nonlinearlity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.3809, -0.4591, -0.2012, -0.0889,  0.0208,  0.6275, -0.6029,  0.0941,\n",
      "         -0.5884,  0.7760, -0.3774, -0.3122, -0.2136,  0.1389,  0.2838, -0.0772,\n",
      "         -0.1289,  0.2597, -0.1792, -0.2953],\n",
      "        [-0.2087, -0.4531, -0.1286,  0.1053, -0.0609,  0.7591, -0.3363,  0.0673,\n",
      "         -0.3959,  0.5053, -0.1523, -0.4117, -0.2573,  0.2128,  0.2217, -0.0750,\n",
      "         -0.4974,  0.2608,  0.0301, -0.2678],\n",
      "        [ 0.0622, -0.3622,  0.2625, -0.0342, -0.2117,  0.4093, -0.7744,  0.3127,\n",
      "         -0.5080,  0.2088, -0.3570, -0.2558, -0.6534,  0.2453,  0.3145, -0.1349,\n",
      "         -0.6084, -0.1633, -0.1698, -0.0237]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0208, 0.6275, 0.0000, 0.0941, 0.0000,\n",
      "         0.7760, 0.0000, 0.0000, 0.0000, 0.1389, 0.2838, 0.0000, 0.0000, 0.2597,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1053, 0.0000, 0.7591, 0.0000, 0.0673, 0.0000,\n",
      "         0.5053, 0.0000, 0.0000, 0.0000, 0.2128, 0.2217, 0.0000, 0.0000, 0.2608,\n",
      "         0.0301, 0.0000],\n",
      "        [0.0622, 0.0000, 0.2625, 0.0000, 0.0000, 0.4093, 0.0000, 0.3127, 0.0000,\n",
      "         0.2088, 0.0000, 0.0000, 0.0000, 0.2453, 0.3145, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "- An ordered container of modules. \n",
    "- The data is passed through all the modules in the same order as defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten, \n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "- The last linear layer of the neural network return logits - raw values in [-inf, inf].\n",
    "- The Softmax function takes this output from the last linear layer and scale the value to [0,1] representing the models's predicted probabilites for each class. \n",
    "- dim parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "- Many layers inside a neural network are parameterized. (Have associated weights and biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0315,  0.0264,  0.0074,  ..., -0.0206,  0.0258, -0.0332],\n",
      "        [ 0.0039, -0.0018, -0.0003,  ..., -0.0356,  0.0011, -0.0187]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0130,  0.0163], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0044,  0.0199, -0.0252,  ...,  0.0302,  0.0240,  0.0369],\n",
      "        [ 0.0376,  0.0113,  0.0169,  ..., -0.0179, -0.0099,  0.0024]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0058, 0.0235], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0167, -0.0363,  0.0027,  ..., -0.0290,  0.0372,  0.0349],\n",
      "        [ 0.0162,  0.0313, -0.0146,  ...,  0.0374, -0.0323, -0.0045]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0243, -0.0025], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd\n",
    "\n",
    "- The most frequently used algorithm is back propagation where the parameters (model weights) are adjusted according to the gradient of the loss function.\n",
    "- PyTorch has a built-in differentiation engin called torch.autograd which supports automatic computation of gradient for any computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple one layer neural network\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad= True) # weight (One of the parameters that needs to be optimized so requires_grad is set to True)\n",
    "b = torch.randn(3, requires_grad = True)  # Bias (Another parameter that needs to be optimized)\n",
    "z = torch.matmul(x,w) + b # output\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) #loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients\n",
    "\n",
    "- To optimize weights for parameters i a neural network, we need to compute the derivatives of our loss function with respect to parameters. \n",
    "- To compute those derivatives we call loss.backward(), then retrieve the values from w.grad and b.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0475, 0.3162, 0.2015],\n",
      "        [0.0475, 0.3162, 0.2015],\n",
      "        [0.0475, 0.3162, 0.2015],\n",
      "        [0.0475, 0.3162, 0.2015],\n",
      "        [0.0475, 0.3162, 0.2015]])\n",
      "tensor([0.0475, 0.3162, 0.2015])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking\n",
    "- There are times in which we do not want to track the gradient and all tensors with the requires_grad is defaulted to True. Times can be if the model is already trained and want to just do forward computation. \n",
    "- One way to disable gradient tracking is surounding computation code with torch.no_grad() block or using the detach() method\n",
    "\n",
    "- Some reasons to not track gradient is setting some parameters as frozen parameters and also speed up computation when you are doing only forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# torch.no_grad() block\n",
    "\n",
    "z = torch.matmul(x, w)+b #Gradient tracked\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad(): #Gradient not tracked\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd keeps a record of data (tensors) and executed operations (along with the resulting new tensors) in a directed acyclic graph consisting of Function objects. Where the leaves are the input tensors and the roots are the output tensors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "- Hyperparameters are adjustable parameters that let you control the model optimization process. \n",
    "\n",
    "#### Examples of some Hyperparameters\n",
    "- Number of Epochs - the number of times to iterate over the dataset\n",
    "- Batch Size - The number of data samples propagated through the network before the parameters are updated\n",
    "- Learning Rate - How much to update the models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Loop\n",
    "- Each iteration of the optimization loop is called an epoch\n",
    "\n",
    "#### Epochs consists of two main parts:\n",
    "- __Train Loop__ : Iterate over the training dataset and try to converge to optimal parameters.\n",
    "- __Validation/Test Loop__ : Iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "### Loss Function\n",
    "- The loss function is used to measure the difference between the result and the target value.\n",
    "- The loss function is also the function that we want ot minimize during training.\n",
    "- Some common loss functions are nn.MSELoss (Mean Square Error) for regression tast, nn.NLLLoss (Negative Log Likelihood) for classification nn.CrossEntropyLoss combines nn.LogSoftmax and NLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- Optimization is adjusting model parameters to reduce the model error in accordance to the loss function. \n",
    "\n",
    "#### Optimization Steps:\n",
    "- Call optimizer.zero_grad() to reset the gradients of model paramters. \n",
    "- Backpropagate the prediction loss with a call to loss.backward(). \n",
    "- Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.293339  [   64/60000]\n",
      "loss: 2.288450  [ 6464/60000]\n",
      "loss: 2.265943  [12864/60000]\n",
      "loss: 2.268883  [19264/60000]\n",
      "loss: 2.239414  [25664/60000]\n",
      "loss: 2.211805  [32064/60000]\n",
      "loss: 2.224590  [38464/60000]\n",
      "loss: 2.190962  [44864/60000]\n",
      "loss: 2.187872  [51264/60000]\n",
      "loss: 2.160578  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 2.152150 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.160277  [   64/60000]\n",
      "loss: 2.152103  [ 6464/60000]\n",
      "loss: 2.090964  [12864/60000]\n",
      "loss: 2.110544  [19264/60000]\n",
      "loss: 2.050568  [25664/60000]\n",
      "loss: 1.994986  [32064/60000]\n",
      "loss: 2.026690  [38464/60000]\n",
      "loss: 1.951298  [44864/60000]\n",
      "loss: 1.958513  [51264/60000]\n",
      "loss: 1.880218  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 1.881582 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.918228  [   64/60000]\n",
      "loss: 1.886666  [ 6464/60000]\n",
      "loss: 1.767148  [12864/60000]\n",
      "loss: 1.804835  [19264/60000]\n",
      "loss: 1.690070  [25664/60000]\n",
      "loss: 1.646776  [32064/60000]\n",
      "loss: 1.668184  [38464/60000]\n",
      "loss: 1.577810  [44864/60000]\n",
      "loss: 1.597844  [51264/60000]\n",
      "loss: 1.487712  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.512755 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.584982  [   64/60000]\n",
      "loss: 1.549275  [ 6464/60000]\n",
      "loss: 1.398651  [12864/60000]\n",
      "loss: 1.463551  [19264/60000]\n",
      "loss: 1.339987  [25664/60000]\n",
      "loss: 1.343657  [32064/60000]\n",
      "loss: 1.356650  [38464/60000]\n",
      "loss: 1.289445  [44864/60000]\n",
      "loss: 1.315926  [51264/60000]\n",
      "loss: 1.218500  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.248765 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.330385  [   64/60000]\n",
      "loss: 1.312370  [ 6464/60000]\n",
      "loss: 1.145876  [12864/60000]\n",
      "loss: 1.244301  [19264/60000]\n",
      "loss: 1.116112  [25664/60000]\n",
      "loss: 1.146952  [32064/60000]\n",
      "loss: 1.169489  [38464/60000]\n",
      "loss: 1.112986  [44864/60000]\n",
      "loss: 1.143819  [51264/60000]\n",
      "loss: 1.064941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.088574 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.162229  [   64/60000]\n",
      "loss: 1.166634  [ 6464/60000]\n",
      "loss: 0.983105  [12864/60000]\n",
      "loss: 1.110440  [19264/60000]\n",
      "loss: 0.981752  [25664/60000]\n",
      "loss: 1.016319  [32064/60000]\n",
      "loss: 1.055881  [38464/60000]\n",
      "loss: 1.001773  [44864/60000]\n",
      "loss: 1.032623  [51264/60000]\n",
      "loss: 0.969954  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.986149 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.045609  [   64/60000]\n",
      "loss: 1.073191  [ 6464/60000]\n",
      "loss: 0.871778  [12864/60000]\n",
      "loss: 1.021890  [19264/60000]\n",
      "loss: 0.897711  [25664/60000]\n",
      "loss: 0.924086  [32064/60000]\n",
      "loss: 0.981678  [38464/60000]\n",
      "loss: 0.928819  [44864/60000]\n",
      "loss: 0.955009  [51264/60000]\n",
      "loss: 0.905778  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.915837 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.959075  [   64/60000]\n",
      "loss: 1.007613  [ 6464/60000]\n",
      "loss: 0.791283  [12864/60000]\n",
      "loss: 0.958124  [19264/60000]\n",
      "loss: 0.841385  [25664/60000]\n",
      "loss: 0.855735  [32064/60000]\n",
      "loss: 0.928975  [38464/60000]\n",
      "loss: 0.879047  [44864/60000]\n",
      "loss: 0.898163  [51264/60000]\n",
      "loss: 0.858575  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.864544 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.892251  [   64/60000]\n",
      "loss: 0.957787  [ 6464/60000]\n",
      "loss: 0.730272  [12864/60000]\n",
      "loss: 0.909402  [19264/60000]\n",
      "loss: 0.800674  [25664/60000]\n",
      "loss: 0.803687  [32064/60000]\n",
      "loss: 0.888714  [38464/60000]\n",
      "loss: 0.843894  [44864/60000]\n",
      "loss: 0.855139  [51264/60000]\n",
      "loss: 0.821587  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.825285 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.838858  [   64/60000]\n",
      "loss: 0.917228  [ 6464/60000]\n",
      "loss: 0.682409  [12864/60000]\n",
      "loss: 0.870753  [19264/60000]\n",
      "loss: 0.769490  [25664/60000]\n",
      "loss: 0.763196  [32064/60000]\n",
      "loss: 0.855881  [38464/60000]\n",
      "loss: 0.817958  [44864/60000]\n",
      "loss: 0.821525  [51264/60000]\n",
      "loss: 0.791609  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.793932 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load the Model\n",
    "\n",
    "#### Saving and Loading Model Weights\n",
    "- PyTorch models store the learned parameters in an internal state dictionary, called state_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = models.vgg16(weights='IMAGENET1K_V1')\\ntorch.save(model.state_dict(), 'model_weights.pth')\\n\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "'''\n",
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvgg16\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "model = models.vgg16\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models with Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
